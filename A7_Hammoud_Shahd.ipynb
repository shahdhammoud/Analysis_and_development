{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T12:45:41.482747Z",
     "start_time": "2025-10-12T12:45:39.076512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math,heapq,collections,requests,io,re,struct,random\n",
    "import numpy as np\n",
    "\n",
    "GUTEN_URL=\"https://www.gutenberg.org/cache/epub/19/pg19.txt\"\n",
    "r=requests.get(GUTEN_URL,timeout=10); r.raise_for_status()\n",
    "raw=r.text\n",
    "m=re.search(r\"THE SONG OF HIAWATHA(.*)End of the Project Gutenberg EBook\",raw,flags=re.S|re.I)\n",
    "if m:\n",
    "    poem_text=m.group(1)\n",
    "else:\n",
    "    poem_text=raw\n",
    "chapters=re.split(r\"\\n{2,}\",poem_text.strip())\n",
    "chosen=None\n",
    "for block in chapters:\n",
    "    if \"Hiawatha\" in block and len(block)>500:\n",
    "        chosen=block; break\n",
    "if chosen is None:\n",
    "    chosen=poem_text[:4000]\n",
    "text=re.sub(r\"\\r\\n\", \"\\n\", chosen).strip()\n",
    "data=text.encode(\"ascii\",\"ignore\")\n",
    "if len(data)==0:\n",
    "    raise RuntimeError(\"No ASCII text extracted\")\n",
    "\n",
    "class HuffNode:\n",
    "    def __init__(self,freq,symbol=None,left=None,right=None):\n",
    "        self.freq=freq; self.symbol=symbol; self.left=left; self.right=right\n",
    "    def __lt__(self,other):\n",
    "        return self.freq<other.freq\n",
    "\n",
    "freq=collections.Counter(data)\n",
    "heap=[HuffNode(f,s) for s,f in freq.items()]\n",
    "heapq.heapify(heap)\n",
    "if len(heap)==0:\n",
    "    huffman_bits_len=0; huffman_codes={}\n",
    "else:\n",
    "    if len(heap)==1:\n",
    "        node=heapq.heappop(heap)\n",
    "        huffman_codes={node.symbol:\"0\"}\n",
    "        huffman_bits_len=len(data)*1\n",
    "    else:\n",
    "        while len(heap)>1:\n",
    "            a=heapq.heappop(heap); b=heapq.heappop(heap)\n",
    "            heapq.heappush(heap,HuffNode(a.freq+b.freq,None,a,b))\n",
    "        root=heap[0]\n",
    "        huffman_codes={}\n",
    "        def walk(node,prefix=\"\"):\n",
    "            if node is None:\n",
    "                return\n",
    "            if node.symbol is not None:\n",
    "                huffman_codes[node.symbol]=prefix if prefix!=\"\" else \"0\"\n",
    "            else:\n",
    "                walk(node.left,prefix+\"0\"); walk(node.right,prefix+\"1\")\n",
    "        walk(root,\"\")\n",
    "        huffman_bits_len=sum(len(huffman_codes[b]) for b in data)\n",
    "huffman_table_size_bits=sum(8+len(v) for v in huffman_codes.values())\n",
    "\n",
    "def huffman_encode(data,codes):\n",
    "    out_bits=[]\n",
    "    for b in data:\n",
    "        out_bits.append(codes[b])\n",
    "    return \"\".join(out_bits)\n",
    "\n",
    "def huffman_decode(bitstr,root):\n",
    "    out=bytearray()\n",
    "    node=root\n",
    "    for bit in bitstr:\n",
    "        node=node.left if bit==\"0\" else node.right\n",
    "        if node.symbol is not None:\n",
    "            out.append(node.symbol); node=root\n",
    "    return bytes(out)\n",
    "\n",
    "max_table_size=4096\n",
    "def lzw_encode_bytes(data):\n",
    "    table={bytes([i]):i for i in range(256)}\n",
    "    code=256\n",
    "    w=b\"\"\n",
    "    out=[]\n",
    "    for c in data:\n",
    "        wc=w+bytes([c])\n",
    "        if wc in table:\n",
    "            w=wc\n",
    "        else:\n",
    "            out.append(table[w])\n",
    "            if code<max_table_size:\n",
    "                table[wc]=code; code+=1\n",
    "            w=bytes([c])\n",
    "    if w:\n",
    "        out.append(table[w])\n",
    "    width=1\n",
    "    if out:\n",
    "        width=math.ceil(math.log2(max(out)+1))\n",
    "    total_bits=len(out)*width\n",
    "    return out,total_bits,len(table)\n",
    "\n",
    "lzw_codes,lzw_bits_len,lzw_table_entries=lzw_encode_bytes(data)\n",
    "ascii_bits=8*len(data)\n",
    "huffman_ratio=None if ascii_bits==0 else huffman_bits_len/ascii_bits\n",
    "lzw_ratio=None if ascii_bits==0 else lzw_bits_len/ascii_bits\n",
    "\n",
    "print(f\"Original ascii bits: {ascii_bits}\")\n",
    "print(f\"Huffman bits: {huffman_bits_len}\")\n",
    "print(f\"Huffman ratio: {huffman_ratio}\")\n",
    "print(f\"Huffman table size bits (approx): {huffman_table_size_bits}\")\n",
    "print(f\"LZW bits (approx): {lzw_bits_len}\")\n",
    "print(f\"LZW ratio: {lzw_ratio}\")\n",
    "print(f\"LZW dictionary entries: {lzw_table_entries}\")\n",
    "\n",
    "print(\"\\nTop 20 Huffman codes by frequency:\")\n",
    "items=sorted(huffman_codes.items(), key=lambda kv:-freq[kv[0]])[:20]\n",
    "for b,code in items:\n",
    "    ch=bytes([b]).decode(\"ascii\",\"replace\")\n",
    "    print(f\"{repr(ch)} ({b}) : {code}\")\n",
    "\n",
    "print(\"\\nFirst 80 LZW output codes:\")\n",
    "print(lzw_codes[:80])\n"
   ],
   "id": "c197c097738d3913",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ascii bits: 1498224\n",
      "Huffman bits: 862721\n",
      "Huffman ratio: 0.5758291150054998\n",
      "Huffman table size bits (approx): 1327\n",
      "LZW bits (approx): 743016\n",
      "LZW ratio: 0.4959311825201038\n",
      "LZW dictionary entries: 4096\n",
      "\n",
      "Top 20 Huffman codes by frequency:\n",
      "' ' (32) : 110\n",
      "'e' (101) : 001\n",
      "'t' (116) : 1001\n",
      "'a' (97) : 1000\n",
      "'h' (104) : 0110\n",
      "'n' (110) : 0101\n",
      "'o' (111) : 0100\n",
      "'i' (105) : 0001\n",
      "'s' (115) : 0000\n",
      "'r' (114) : 11110\n",
      "'d' (100) : 10110\n",
      "'\\n' (10) : 10100\n",
      "'l' (108) : 01110\n",
      "',' (44) : 111111\n",
      "'w' (119) : 111010\n",
      "'g' (103) : 111001\n",
      "'u' (117) : 111000\n",
      "'m' (109) : 101110\n",
      "'f' (102) : 101010\n",
      "'y' (121) : 1111100\n",
      "\n",
      "First 80 LZW output codes:\n",
      "[84, 104, 105, 115, 32, 101, 98, 111, 111, 107, 32, 258, 32, 102, 111, 114, 32, 116, 104, 101, 32, 117, 115, 275, 111, 102, 32, 97, 110, 121, 111, 110, 275, 283, 121, 119, 274, 114, 275, 105, 110, 272, 274, 32, 85, 110, 105, 116, 101, 100, 32, 83, 116, 97, 303, 259, 283, 100, 10, 109, 111, 115, 116, 32, 111, 273, 101, 271, 112, 97, 114, 116, 259, 280, 297, 275, 119, 270, 108, 305]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LZW was better: it compressed the poem to ~49.6% of the original versus Huffman’s ~57.6% because LZW learns and reuses repeated multi-symbol substrings (words and common phrases) across the text while Huffman only optimizes individual-symbol codes, so LZW captures higher-order redundancy beyond the per-symbol entropy that limits Huffman./",
   "id": "aad392b11d0b7b3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "6. <h3>Try to invent and explain an algorithm for embedding the name of a state (from\n",
    "Afghanistan to Zimbabwe https://en.wikipedia.org/wiki/List_of_sovereign_states)\n",
    "in some world-wide data-set for some ML task. No code required. Only idea and its\n",
    "brief description."
   ],
   "id": "d4f377346cb2eecc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Method: content-aware semantic watermark via controlled label-conditional augmentation:\n",
    "\n",
    "choose a small subset of records across geographically diverse source partitions.\n",
    "\n",
    "select a robust natural-language transformation mapping: for textual fields, insert a short, semantically consistent phrase derived deterministically from the state name via keyed reversible mapping (example: map each character to a word from a curated synonym list, producing a terse phrase of 3–6 words that reads plausibly).\n",
    "\n",
    "apply this phrase only in contexts where it is syntactically natural (e.g., as a location qualifier, a comment field, or an auxiliary description), ensuring minimal perturbation to feature distribution.\n",
    "\n",
    "keep embedding frequency low and spread geographically to avoid introducing class/feature imbalance; record the mapping key privately.\n",
    "\n",
    "detection/extraction: given the private key, transform dataset fields using the same deterministic mapping and search for matches; presence and frequency of the phrase reliably indicate which state name was embedded.\n",
    "\n",
    "The phrase remains human-plausible so downstream models and human auditors see no obvious artifact; deterministic keyed mapping makes detection robust to noise and prevents accidental collisions across different keys or runs."
   ],
   "id": "84d2a0524793842e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dc07d21c9e7da7c7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
